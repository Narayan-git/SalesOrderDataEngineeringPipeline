{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SportsBar Incremental Order Fact Loading Pipeline\n",
    "## Medallion Architecture: Daily Incremental Processing\n",
    "\n",
    "### üìå NOTEBOOK OVERVIEW\n",
    "\n",
    "This notebook processes daily incremental orders from landing zone through medallion layers, applying quality transformations, recalculating monthly aggregates for affected months, and upserting into parent company fact table.\n",
    "\n",
    "**Pipeline Purpose:** Daily ETL job that loads new orders, validates/transforms them, recalculates impacted monthly aggregates, and merges with parent fact table.\n",
    "\n",
    "**Execution Pattern:** Scheduled daily (morning, after orders arrive)\n",
    "\n",
    "**Output:** `fmcg.gold.fact_orders` - Updated with yesterday's orders aggregated to monthly grain\n",
    "\n",
    "### üîÑ INCREMENTAL PROCESSING FLOW\n",
    "\n",
    "| Layer | Scope | Table Name | Pattern | Purpose |\n",
    "|-------|-------|-----------|---------|---------|\n",
    "| **Bronze** | New files only | `fmcg.bronze.orders` | Append | Preserve all order history |\n",
    "| **Silver** | New orders | `fmcg.silver.orders` | Merge | Quality & standardization |\n",
    "| **Gold (Staging)** | Daily records | `fmcg.gold.sb_fact_orders` | Merge | Raw fact records |\n",
    "| **Gold (Parent)** | Monthly aggregate | `fmcg.gold.fact_orders` | Merge | Recalculate affected months |\n",
    "\n",
    "### üéØ KEY DIFFERENCES FROM FULL LOAD\n",
    "\n",
    "**Full Load (1_full_load_fact.ipynb):**\n",
    "- Runs once to bootstrap pipeline\n",
    "- Processes all historical order files\n",
    "- Loads everything to monthly grain\n",
    "- Initializes parent fact table\n",
    "\n",
    "**Incremental Load (This notebook):**\n",
    "- Runs daily as scheduled job\n",
    "- Processes only new order files (landing/ folder)\n",
    "- Archives processed files (processed/ folder)\n",
    "- Recalculates only affected months\n",
    "- Merges into existing parent data (preserves history)\n",
    "\n",
    "### üí° INCREMENTAL LOAD STRATEGY\n",
    "\n",
    "1. **Read new files** from `orders/landing/*.csv` to bronze\n",
    "2. **Transform daily records** through silver quality checks\n",
    "3. **Merge into fact** to capture individual order records\n",
    "4. **Identify affected months** (months with new data)\n",
    "5. **Recalculate aggregates** only for affected months\n",
    "6. **Upsert to parent** maintaining historical monthly data\n",
    "7. **Archive files** from landing to processed (prevents reprocessing)\n",
    "8. **Cleanup staging** tables (temporary tables dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b61ab0d-a8ce-4ff2-a9a2-10a40fac6f8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## STEP 1: Import Required Libraries\n",
    "\n",
    "PySpark SQL functions and Delta Lake modules for incremental ETL processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfd83738-4dee-469d-ba19-41842aa8140d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import PySpark SQL functions for transformations\n",
    "from pyspark.sql import functions as F\n",
    "# Import DeltaTable for MERGE operations (upsert patterns)\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89f91606-ac90-4f4f-b8f4-9c7f6118f7f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## STEP 2: Load Project Utilities & Initialize Notebook Widgets\n",
    "\n",
    "Import configuration and set up pipeline parameters for daily execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6ac9f27-de74-43bc-82aa-9872f9c083a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load utilities - defines schema name constants\n",
    "%run /Workspace/Project1/1_setup_catalog/utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33315035-37e0-495f-ae7d-ea5f80703cfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bronze silver gold\n"
     ]
    }
   ],
   "source": [
    "# Verify schema constants loaded correctly\n",
    "print(bronze_schema, silver_schema, gold_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a802c210-a0a4-4a93-8b85-a4b5f9a4bfad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Path:  abfss://conatiner-de-practice@adlsgen2narayan.dfs.core.windows.net/orders\n",
      "Landing Path:  abfss://conatiner-de-practice@adlsgen2narayan.dfs.core.windows.net/orders/landing/\n",
      "Processed Path:  abfss://conatiner-de-practice@adlsgen2narayan.dfs.core.windows.net/orders/processed/\n"
     ]
    }
   ],
   "source": [
    "# Configure notebook widgets\n",
    "dbutils.widgets.text(\"catalog\", \"fmcg\", \"Catalog\")\n",
    "dbutils.widgets.text(\"data_source\", \"orders\", \"Data Source\")\n",
    "\n",
    "# Get widget values\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "data_source = dbutils.widgets.get(\"data_source\")\n",
    "\n",
    "# Configure directory paths for incremental processing\n",
    "base_path = f\"abfss://conatiner-de-practice@adlsgen2narayan.dfs.core.windows.net/{data_source}\"\n",
    "landing_path = f\"{base_path}/landing/\"      # NEW files to process\n",
    "processed_path = f\"{base_path}/processed/\"  # Archive of processed files (prevents reprocessing)\n",
    "print(\"Base Path: \", base_path)\n",
    "print(\"Landing Path: \", landing_path)\n",
    "print(\"Processed Path: \", processed_path)\n",
    "\n",
    "# Define table references for medallion layers\n",
    "bronze_table = f\"{catalog}.{bronze_schema}.{data_source}\"\n",
    "silver_table = f\"{catalog}.{silver_schema}.{data_source}\"\n",
    "gold_table = f\"{catalog}.{gold_schema}.sb_fact_{data_source}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cc96661-2596-4733-be4a-e3d78c28ec05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## STEP 3: BRONZE LAYER - Incremental Order Ingestion\n",
    "\n",
    "**Purpose:** Append new daily orders with metadata tracking\n",
    "\n",
    "**Medallion Pattern:** Raw data capture with full lineage\n",
    "\n",
    "**Key Metadata:**\n",
    "- `read_timestamp` - Processing time\n",
    "- `file_name` - Source file\n",
    "- `file_size` - Data volume\n",
    "\n",
    "**Mode:** APPEND (preserves entire order history)\n",
    "\n",
    "**Output Table:** `fmcg.bronze.orders` (cumulative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47295cfd-41e3-4245-98e8-af5c83b6e76e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows:  9854\n",
      "+-------------+--------------------+-----------+----------+---------+--------------------+--------------------+---------+\n",
      "|     order_id|order_placement_date|customer_id|product_id|order_qty|      read_timestamp|           file_name|file_size|\n",
      "+-------------+--------------------+-----------+----------+---------+--------------------+--------------------+---------+\n",
      "|FDEC818102602|Tuesday, December...|     789102|  25891502|    196.0|2025-11-30 16:21:...|orders_2025_12_16...|    23060|\n",
      "|FDEC818102602|Tuesday, December...|     789102|  25891503|     NULL|2025-11-30 16:21:...|orders_2025_12_16...|    23060|\n",
      "|FDEC818102602|Tuesday, December...|     789102|  25891602|    147.0|2025-11-30 16:21:...|orders_2025_12_16...|    23060|\n",
      "|FDEC818102602|Tuesday, December...|     789102|  25891101|    337.0|2025-11-30 16:21:...|orders_2025_12_16...|    23060|\n",
      "|FDEC818102602|Tuesday, December...|    INVALID|  25891202|    202.0|2025-11-30 16:21:...|orders_2025_12_16...|    23060|\n",
      "+-------------+--------------------+-----------+----------+---------+--------------------+--------------------+---------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Read NEW orders from landing zone (only today's files)\n",
    "df = (\n",
    "    spark.read.options(header=True, inferSchema=True)\n",
    "    .csv(f\"{landing_path}/*.csv\")               # Read all CSVs in landing folder\n",
    "    .withColumn(\"read_timestamp\", F.current_timestamp())  # Add processing time\n",
    "    .select(\"*\", \"_metadata.file_name\", \"_metadata.file_size\")  # Include file metadata\n",
    ")\n",
    "\n",
    "# Report record count for monitoring\n",
    "print(\"Total Rows: \", df.count())\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "327790a0-6364-4880-8efe-72d189b7dd2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Append new orders to bronze table (cumulative, never overwrite)\n",
    "df.write\\\n",
    " .format(\"delta\") \\\n",
    " .option(\"delta.enableChangeDataFeed\", \"true\") \\\n",
    " .mode(\"append\") \\                               # APPEND mode: preserves history\n",
    " .saveAsTable(bronze_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a93dded-a69e-44c9-909d-7c64f31caa76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create Staging Table for Just-Arrived Incremental Data\n",
    "\n",
    "**Purpose:** Isolate today's orders for transformation pipeline\n",
    "\n",
    "**Benefit:** Enables tracking of which orders are new vs historical\n",
    "\n",
    "**Table:** `fmcg.bronze.staging_orders` (OVERWRITE each day with new data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3096c71a-7d5b-42f3-92de-58f1273c7f5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create staging table with ONLY today's arrived orders (not full bronze history)\n",
    "# This allows us to track exactly which records are new vs which existed before\n",
    "df.write\\\n",
    " .format(\"delta\") \\\n",
    " .option(\"delta.enableChangeDataFeed\", \"true\") \\\n",
    " .mode(\"overwrite\") \\                            # Overwrite daily (reset staging)\n",
    " .saveAsTable(f\"{catalog}.{bronze_schema}.staging_{data_source}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8693a838-c546-4844-823f-d500350deead",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Archive Processed Files (Prevents Reprocessing)\n",
    "\n",
    "**Purpose:** Move files from landing ‚Üí processed after ingestion\n",
    "\n",
    "**Benefit:** Idempotent design - safe to retry without duplicate loading\n",
    "\n",
    "**Process:** MOVE operation (atomic, removes from source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1446dd5d-997d-4932-9b14-699de758b574",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Move processed files from landing/ to processed/ directory\n",
    "# This prevents reprocessing if job reruns and ensures exactly-once delivery semantics\n",
    "files = dbutils.fs.ls(landing_path)\n",
    "for file_info in files:\n",
    "    dbutils.fs.mv(\n",
    "        file_info.path,                          # Source: landing directory\n",
    "        f\"{processed_path}/{file_info.name}\",   # Destination: processed archive\n",
    "        True                                     # Overwrite if exists\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3f44c22-84d9-4ae6-afe7-c42b98a4a1d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## STEP 4: SILVER LAYER - Daily Order Quality & Standardization\n",
    "\n",
    "**Purpose:** Apply data quality transformations to incremental orders\n",
    "\n",
    "**Medallion Pattern:** Clean, validate, and standardize to conformed schema\n",
    "\n",
    "**Key Quality Steps (6-Step Process):**\n",
    "1. ‚úÖ **Null Filter** - Keep only orders with quantities\n",
    "2. ‚úÖ **Customer ID Validation** - Numeric check with fallback\n",
    "3. ‚úÖ **Date Cleaning** - Remove weekday prefix\n",
    "4. ‚úÖ **Multi-Format Date Parsing** - Handle 4 date formats\n",
    "5. ‚úÖ **Deduplication** - Remove exact duplicates\n",
    "6. ‚úÖ **Product Join** - Add product_code for fact joins\n",
    "\n",
    "**Output Table:** `fmcg.silver.orders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f96fa16e-2e19-4e3a-ab14-891f49444d06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-----------+----------+---------+--------------------+--------------------+---------+\n",
      "|    order_id|order_placement_date|customer_id|product_id|order_qty|      read_timestamp|           file_name|file_size|\n",
      "+------------+--------------------+-----------+----------+---------+--------------------+--------------------+---------+\n",
      "|FDEC83622503|Monday, December ...|     789622|  25891302|     39.0|2025-11-30 16:22:...|orders_2025_12_01...|    21062|\n",
      "|FDEC83622503|Monday, December ...|     789622|  25891301|     26.0|2025-11-30 16:22:...|orders_2025_12_01...|    21062|\n",
      "+------------+--------------------+-----------+----------+---------+--------------------+--------------------+---------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "# Read staged orders (today's new orders only) to begin transformations\n",
    "df_orders = spark.sql(f\"SELECT * FROM {catalog}.{bronze_schema}.staging_{data_source};\")\n",
    "df_orders.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2de139f-288e-4415-a99d-4a77a7caf463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Silver Layer Transformations (6-Step Quality Framework)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e9ea758-9008-41a4-9f73-bbac6aed94b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# QUALITY STEP 1Ô∏è‚É£: Keep only rows where order_qty is present\n",
    "# Filter out records without quantities (incomplete orders)\n",
    "df_orders = df_orders.filter(F.col(\"order_qty\").isNotNull())\n",
    "\n",
    "# QUALITY STEP 2Ô∏è‚É£: Clean customer_id ‚Üí keep numeric, else set to 999999\n",
    "# Validates that customer_id matches expected format; fallback for invalid values\n",
    "df_orders = df_orders.withColumn(\n",
    "    \"customer_id\",\n",
    "    F.when(F.col(\"customer_id\").rlike(\"^[0-9]+$\"), F.col(\"customer_id\"))  # Numeric? Keep it\n",
    "     .otherwise(\"999999\")                                                   # Invalid? Use fallback\n",
    "     .cast(\"string\")                                                        # Ensure string type\n",
    ")\n",
    "\n",
    "# QUALITY STEP 3Ô∏è‚É£: Remove weekday name from the date text\n",
    "# Pattern: \"Tuesday, July 01, 2025\" ‚Üí \"July 01, 2025\"\n",
    "# Regex replaces leading weekday followed by comma and space\n",
    "df_orders = df_orders.withColumn(\n",
    "    \"order_placement_date\",\n",
    "    F.regexp_replace(F.col(\"order_placement_date\"), r\"^[A-Za-z]+,\\s*\", \"\")\n",
    ")\n",
    "\n",
    "# QUALITY STEP 4Ô∏è‚É£: Parse order_placement_date using multiple possible formats\n",
    "# Try formats sequentially; coalesce returns first non-null result\n",
    "df_orders = df_orders.withColumn(\n",
    "    \"order_placement_date\",\n",
    "    F.coalesce(\n",
    "        F.try_to_date(\"order_placement_date\", \"yyyy/MM/dd\"),   # Format 1\n",
    "        F.try_to_date(\"order_placement_date\", \"dd-MM-yyyy\"),   # Format 2\n",
    "        F.try_to_date(\"order_placement_date\", \"dd/MM/yyyy\"),   # Format 3\n",
    "        F.try_to_date(\"order_placement_date\", \"MMMM dd, yyyy\"), # Format 4 (month name)\n",
    "    )\n",
    ")\n",
    "\n",
    "# QUALITY STEP 5Ô∏è‚É£: Drop exact duplicates by order composition\n",
    "# Business key: order_id + date + customer + product + quantity (identifies same order)\n",
    "df_orders = df_orders.dropDuplicates(\n",
    "    [\"order_id\", \"order_placement_date\", \"customer_id\", \"product_id\", \"order_qty\"]\n",
    ")\n",
    "\n",
    "# Convert product_id to string for join consistency with products table\n",
    "df_orders = df_orders.withColumn('product_id', F.col('product_id').cast('string'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6acc6763-0207-4856-a62b-b33c4a86f1e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|  min_date|  max_date|\n",
      "+----------+----------+\n",
      "|2025-12-01|2025-12-30|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify date parsing worked - check min/max dates in dataset\n",
    "df_orders.agg(\n",
    "    F.min(\"order_placement_date\").alias(\"min_date\"),\n",
    "    F.max(\"order_placement_date\").alias(\"max_date\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0988dcd1-85cd-49fe-afb8-182ea25da306",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUALITY STEP 6Ô∏è‚É£: Join with Products Dimension\n",
    "\n",
    "**Purpose:** Add product_code (conformed key) for fact table joins\n",
    "\n",
    "**Method:** Inner join on product_id (ensures valid products only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad6d70e7-fd58-43c6-b571-304b422e425b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+-----------+----------+---------+--------------------+--------------------+---------+--------------------+\n",
      "|     order_id|order_placement_date|customer_id|product_id|order_qty|      read_timestamp|           file_name|file_size|        product_code|\n",
      "+-------------+--------------------+-----------+----------+---------+--------------------+--------------------+---------+--------------------+\n",
      "| FDEC84420202|          2025-12-01|     999999|  25891201|    458.0|2025-11-30 16:22:...|orders_2025_12_01...|    21062|2e387cef1424d6e7b...|\n",
      "| FDEC87522503|          2025-12-04|     789522|  25891403|    342.0|2025-11-30 16:22:...|orders_2025_12_04...|    22385|77b6f538a9d0e0cf8...|\n",
      "| FDEC89522601|          2025-12-08|     789522|  25891403|    476.0|2025-11-30 16:22:...|orders_2025_12_08...|    21711|77b6f538a9d0e0cf8...|\n",
      "|FDEC817203502|          2025-12-15|     789203|  25891203|    300.0|2025-11-30 16:22:...|orders_2025_12_15...|    20287|889c67757ece9c973...|\n",
      "|FDEC818101501|          2025-12-16|     789101|  25891501|    221.0|2025-11-30 16:22:...|orders_2025_12_16...|    23060|ee1f7df9cf660ef02...|\n",
      "+-------------+--------------------+-----------+----------+---------+--------------------+--------------------+---------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Load products dimension from silver layer (contains validated product_codes)\n",
    "df_products = spark.table(\"fmcg.silver.products\")\n",
    "# Join orders with products to get standardized product_code\n",
    "df_joined = df_orders.join(\n",
    "    df_products,                                 # Right table: products\n",
    "    on=\"product_id\",                             # Join key: product_id\n",
    "    how=\"inner\"                                  # Inner join: keep only matched products\n",
    ").select(df_orders[\"*\"], df_products[\"product_code\"])  # Keep all order columns + product_code\n",
    "\n",
    "df_joined.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5999f375-a9fc-465f-9a3f-29ea572fbdf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write to silver orders table\n",
    "# If table doesn't exist, create it; otherwise, merge new records\n",
    "if not (spark.catalog.tableExists(silver_table)):\n",
    "    # First time: create new silver table\n",
    "    df_joined.write.format(\"delta\").option(\n",
    "        \"delta.enableChangeDataFeed\", \"true\"\n",
    "    ).option(\"mergeSchema\", \"true\").mode(\"overwrite\").saveAsTable(silver_table)\n",
    "else:\n",
    "    # Incremental: merge new orders into existing table (avoid duplicates)\n",
    "    silver_delta = DeltaTable.forName(spark, silver_table)\n",
    "    silver_delta.alias(\"silver\").merge(\n",
    "        df_joined.alias(\"bronze\"),\n",
    "        # Match condition: same order with same date/customer/product/qty\n",
    "        \"silver.order_placement_date = bronze.order_placement_date AND silver.order_id = bronze.order_id AND silver.product_code = bronze.product_code AND silver.customer_id = bronze.customer_id\"\n",
    "    ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4d4ef25-ebfa-4033-a212-d9a3813b3854",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create Silver Staging Table (Today's Transformed Orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be42c312-3459-4115-a140-b46cca067dd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create staging table with only today's transformed orders for downstream processing\n",
    "df_joined.write\\\n",
    " .format(\"delta\") \\\n",
    " .option(\"delta.enableChangeDataFeed\", \"true\") \\\n",
    " .mode(\"overwrite\") \\                             # Overwrite daily (reset staging)\n",
    " .saveAsTable(f\"{catalog}.{silver_schema}.staging_{data_source}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81b95615-a9db-460d-a7fb-6dae642a7d71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## STEP 5: GOLD LAYER - Daily Order Facts (Raw Grain)\n",
    "\n",
    "**Purpose:** Store individual order records with daily grain (not aggregated)\n",
    "\n",
    "**Medallion Pattern:** Select business columns, store at transaction level\n",
    "\n",
    "**Grain:** Daily (one row per order)\n",
    "\n",
    "**Output Table:** `fmcg.gold.sb_fact_orders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db80c5d1-3e4a-4067-9d84-33cd9770fd85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-------------+--------------------+----------+-------------+\n",
      "|    order_id|      date|customer_code|        product_code|product_id|sold_quantity|\n",
      "+------------+----------+-------------+--------------------+----------+-------------+\n",
      "|FDEC84420202|2025-12-01|       999999|2e387cef1424d6e7b...|  25891201|        458.0|\n",
      "|FDEC87522503|2025-12-04|       789522|77b6f538a9d0e0cf8...|  25891403|        342.0|\n",
      "+------------+----------+-------------+--------------------+----------+-------------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "# Read staged orders and prepare for gold layer\n",
    "# Select columns needed for fact table; rename for clarity\n",
    "df_gold = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        order_id,                                   # Order identifier\n",
    "        order_placement_date as date,              # Order date (renamed for clarity)\n",
    "        customer_id as customer_code,              # Customer identifier\n",
    "        product_code,                              # Conformed product key\n",
    "        product_id,                                # Original product ID\n",
    "        order_qty as sold_quantity                 # Order quantity (renamed)\n",
    "    FROM {catalog}.{silver_schema}.staging_{data_source};\n",
    "\"\"\")\n",
    "\n",
    "df_gold.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb3e30ed-eb88-43a7-9356-b73e2cdc582a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7736"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count orders being processed (monitoring metric)\n",
    "df_gold.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be9b174a-eafc-435d-8ad1-207a7160316e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write to gold facts table\n",
    "# If doesn't exist, create; otherwise merge to avoid duplicates\n",
    "if not (spark.catalog.tableExists(gold_table)):\n",
    "    print(\"creating New Table\")\n",
    "    df_gold.write.format(\"delta\").option(\n",
    "        \"delta.enableChangeDataFeed\", \"true\"\n",
    "    ).option(\"mergeSchema\", \"true\").mode(\"overwrite\").saveAsTable(gold_table)\n",
    "else:\n",
    "    gold_delta = DeltaTable.forName(spark, gold_table)\n",
    "    gold_delta.alias(\"source\").merge(\n",
    "        df_gold.alias(\"gold\"),\n",
    "        # Match condition: same order on same date with same customer/product\n",
    "        \"source.date = gold.date AND source.order_id = gold.order_id AND source.product_code = gold.product_code AND source.customer_code = gold.customer_code\"\n",
    "    ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39c1784b-9b14-417d-919b-d49864ace9fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## STEP 6: MERGE with Parent Company Fact Table (CRITICAL - Monthly Aggregation)\n",
    "\n",
    "**Purpose:** Recalculate monthly aggregates for affected months and merge with parent\n",
    "\n",
    "**Challenge:** Incremental data arrives daily, but fact table is monthly grain\n",
    "\n",
    "**Solution:** \n",
    "1. Identify affected months (those with new data)\n",
    "2. Recalculate ONLY those months (not entire history)\n",
    "3. Merge into parent fact table\n",
    "\n",
    "**Pattern:** Delta MERGE for efficient incremental aggregation\n",
    "\n",
    "**Output Table:** `fmcg.gold.fact_orders` (monthly grain, parent dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7fb7bfd-66d1-4844-b400-cadd85f74f72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Incremental Load Strategy: Calculate Only Affected Months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e916f9a-5dda-4642-a1c2-044439c9f853",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|start_month|\n",
      "+-----------+\n",
      "| 2025-12-01|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Identify WHICH MONTHS have new orders\n",
    "# This allows us to recalculate only affected periods (huge performance optimization)\n",
    "\n",
    "df_child = spark.sql(f\"SELECT order_placement_date as date FROM {catalog}.{silver_schema}.staging_{data_source}\")\n",
    "\n",
    "# Extract month boundaries and get distinct months with new data\n",
    "incremental_month_df = df_child.select(\n",
    "    F.trunc(\"date\", \"MM\").alias(\"start_month\")  # Truncate to first of month\n",
    ").distinct()\n",
    "\n",
    "incremental_month_df.show()\n",
    "\n",
    "# Create temp view for use in SQL query\n",
    "incremental_month_df.createOrReplaceTempView(\"incremental_months\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e01401d8-eee3-4f8d-8355-b0e2bc7d3914",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows:  7736\n",
      "+----------+--------------------+-------------+-------------+\n",
      "|      date|        product_code|customer_code|sold_quantity|\n",
      "+----------+--------------------+-------------+-------------+\n",
      "|2025-12-01|2e387cef1424d6e7b...|       999999|        458.0|\n",
      "|2025-12-04|77b6f538a9d0e0cf8...|       789522|        342.0|\n",
      "|2025-12-08|77b6f538a9d0e0cf8...|       789522|        476.0|\n",
      "|2025-12-15|889c67757ece9c973...|       789203|        300.0|\n",
      "|2025-12-16|ee1f7df9cf660ef02...|       789101|        221.0|\n",
      "|2025-12-16|889c67757ece9c973...|       789101|        393.0|\n",
      "|2025-12-21|ee1f7df9cf660ef02...|       789622|        190.0|\n",
      "|2025-12-05|e91ba9d665f90254d...|       789303|        462.0|\n",
      "|2025-12-14|889c67757ece9c973...|       789702|        322.0|\n",
      "|2025-12-23|e91ba9d665f90254d...|       789201|        343.0|\n",
      "+----------+--------------------+-------------+-------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Get ALL orders (historical + new) for affected months only\n",
    "# This ensures we recalculate complete monthly aggregates (not just deltas)\n",
    "\n",
    "monthly_table = spark.sql(f\"\"\"\n",
    "    SELECT date, product_code, customer_code, sold_quantity\n",
    "    FROM {catalog}.{gold_schema}.sb_fact_orders sbf\n",
    "    INNER JOIN incremental_months m\n",
    "        ON trunc(sbf.date, 'MM') = m.start_month  -- Match affected month\n",
    "\"\"\")\n",
    "\n",
    "print(\"Total Rows: \", monthly_table.count())\n",
    "monthly_table.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c40cd2e3-1b16-49b7-8c5b-eefbe25589e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2025-12-01|\n",
      "|2025-12-02|\n",
      "|2025-12-03|\n",
      "|2025-12-04|\n",
      "|2025-12-05|\n",
      "|2025-12-06|\n",
      "|2025-12-07|\n",
      "|2025-12-08|\n",
      "|2025-12-09|\n",
      "|2025-12-10|\n",
      "|2025-12-11|\n",
      "|2025-12-12|\n",
      "|2025-12-13|\n",
      "|2025-12-14|\n",
      "|2025-12-15|\n",
      "|2025-12-16|\n",
      "|2025-12-17|\n",
      "|2025-12-18|\n",
      "|2025-12-19|\n",
      "|2025-12-20|\n",
      "+----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Verify which months are being recalculated\n",
    "monthly_table.select('date').distinct().orderBy('date').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "125cd07d-e35e-41b1-8f39-a9116b3c1dd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------------------------------------------------+-------------+-------------+\n",
      "|date      |product_code                                                    |customer_code|sold_quantity|\n",
      "+----------+----------------------------------------------------------------+-------------+-------------+\n",
      "|2025-12-01|778c2a7aa27bfdb211fd5ece048de80d00fbf3d6924bd908d91054796ba16ab6|789402       |1096.0       |\n",
      "|2025-12-01|778c2a7aa27bfdb211fd5ece048de80d00fbf3d6924bd908d91054796ba16ab6|789503       |1839.0       |\n",
      "|2025-12-01|ee1f7df9cf660ef02c33037d8d6eb94cbefe8e7b84c306e9387f09b0cae0abae|789703       |1759.0       |\n",
      "|2025-12-01|3cab59f05924285270313afcfe40a08983bb03dd88f432e34fc6336914c14345|789103       |686.0        |\n",
      "|2025-12-01|889c67757ece9c973791dfbc2d47b026a3342cc7255e47a3170329d158e897c2|789402       |3340.0       |\n",
      "|2025-12-01|0cb7b2f42657b625f754e833aa1cf6a967be26f17415f5342302ebb0e90c8a28|789321       |3765.0       |\n",
      "|2025-12-01|2e387cef1424d6e7b162b45622d4b1a788d11776e33d05cc8552f4ecd2ea1896|789101       |2857.0       |\n",
      "|2025-12-01|716fa4e54b7894c910180276e0535d49afb25cdcfac09533fb74ae00689e5742|789603       |896.0        |\n",
      "|2025-12-01|0cb7b2f42657b625f754e833aa1cf6a967be26f17415f5342302ebb0e90c8a28|789902       |4737.0       |\n",
      "|2025-12-01|102628255d24304d6bbe0438b1ac992054f262e0814d306d0a34d7356cef3268|789221       |5903.0       |\n",
      "+----------+----------------------------------------------------------------+-------------+-------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Aggregate daily orders to monthly grain\n",
    "# Group by: month_start, product_code, customer_code\n",
    "# Aggregate: SUM(sold_quantity) for each group\n",
    "\n",
    "df_monthly_recalc = (\n",
    "    monthly_table\n",
    "    .withColumn(\"month_start\", F.trunc(\"date\", \"MM\"))  # First day of month\n",
    "    .groupBy(\"month_start\", \"product_code\", \"customer_code\")  # Monthly aggregation key\n",
    "    .agg(F.sum(\"sold_quantity\").alias(\"sold_quantity\"))  # Aggregate quantities\n",
    "    .withColumnRenamed(\"month_start\", \"date\")   # Rename: month_start ‚Üí date = first of month\n",
    ")\n",
    "\n",
    "df_monthly_recalc.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d249abe8-621a-46e1-b25c-3348f8f61f7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "612"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Report monthly aggregation result size\n",
    "df_monthly_recalc.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d385807e-eb00-4631-8bcb-7b0c90315328",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_updated_rows: bigint, num_deleted_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: MERGE recalculated monthly aggregates into parent fact table\n",
    "# This upserts the affected months while preserving all historical months\n",
    "\n",
    "gold_parent_delta = DeltaTable.forName(spark, f\"{catalog}.{gold_schema}.fact_orders\")\n",
    "\n",
    "gold_parent_delta.alias(\"parent_gold\").merge(\n",
    "    df_monthly_recalc.alias(\"child_gold\"),\n",
    "    # Match on: same month, same product, same customer\n",
    "    \"parent_gold.date = child_gold.date AND parent_gold.product_code = child_gold.product_code AND parent_gold.customer_code = child_gold.customer_code\"\n",
    ").whenMatchedUpdateAll(  # If exists: update with recalculated values\n",
    ").whenNotMatchedInsertAll(  # If new: insert complete record\n",
    ").execute()\n",
    "\n",
    "# Result: Parent fact table now has updated aggregates for affected months\n",
    "# Historical months (no new data) remain unchanged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48d9be2a-dd59-4dcc-8b94-6a38e6a50b41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## STEP 7: CLEANUP - Drop Temporary Staging Tables\n",
    "\n",
    "**Purpose:** Clean up temporary tables used only during ETL execution\n",
    "\n",
    "**Benefit:** Prevents confusion about stale staging data on next run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "072fcefb-c6b4-4eab-8a13-44e25c0f8d5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.databricks.empty-table+json": {
       "directive_name": "NoDirective"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- Drop bronze staging table (no longer needed after merge)\n",
    "DROP TABLE IF EXISTS fmcg.bronze.staging_orders;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "636b5256-cf29-4396-876a-afcd327d739a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.databricks.empty-table+json": {
       "directive_name": "NoDirective"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- Drop silver staging table (no longer needed after merge)\n",
    "DROP TABLE IF EXISTS fmcg.silver.staging_orders;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7111648949633384,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2_incremental_load_fact",
   "widgets": {
    "catalog": {
     "currentValue": "fmcg",
     "nuid": "07357a85-f675-48fa-883b-9db2f928668e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "fmcg",
      "label": "Catalog",
      "name": "catalog",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "fmcg",
      "label": "Catalog",
      "name": "catalog",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "data_source": {
     "currentValue": "orders",
     "nuid": "821a1d5a-0d00-43de-a31f-87c32042fa66",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "orders",
      "label": "Data Source",
      "name": "data_source",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "orders",
      "label": "Data Source",
      "name": "data_source",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
